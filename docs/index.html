
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184 Final Project Proposal</title>  
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>
 
<body>
<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2022</h1>
<h1 align="middle">Final Project Deliverable: Teapot in a Stadium</h1>
<h2 align="middle">Shilpa Sathyanathan, David McAllister, James Dai, Ethan Gnibus</h2>
<h4 align="middle"> Project Link (different than before) : https://github.com/mcallisterdavid/oski-184-finalproj</h4>
<h4 align="middle"> Proposal Link: https://github.com/shilpasath/cs184-finalproj</h4>	
<h4 align="middle"> Milestone Link: https://github.com/shilpasath/cs184-finalproj-milestone</h4>		

<div>
<h2 align="middle">Abstract</h2>
	<p>
	The goal of our final project was to add AR visuals to stadium performances and football games while learning about computer vision. We had a video of the field and a tabletop aerial view of the field to test with. We developed an iOS application essentially started from scratch since this is a very atypical use case for Apple’s ARKit.To accomplish real-time performance, we wrote all the heavy computation for the GPU and were forced to be clever with how we parallelized each of our image analysis steps. We developed a series of GPU kernel functions to pull lines out of each image. We then classified each pixel. Next, we identified relevant points on the field. Finally, we implemented visuals including a teapot in a stadium, teddy bear, and others. 
	</p>

<h3 align="middle">Technical Approach</h3>
	<p>
	We began by trying to use Hough Line Transform to identify lines in our frames, but this was too expensive and noisy when we went from tabletop demos to the field demo.
	We noticed that the field had lots of rich segmentation information, so we developed a series of GPU kernel functions to pull this out of each image. Using logistic regression, we were able to classify each pixel as green, white, or gold. Then, we looked at these pixels’ proximity to each other to identify field lines and make guesses about their locations on the field.
	After some denoising, we got some reasonable outputs (yellow filter, greenwhiteclean filter side by side). We raycasted from the Cal Logo to identify points on the field. This let us map a set of 2D pixel coordinates to their 3D world coordinates. Going from this data to a transformation matrix is called the perspective-three-point problem, which we solved using a public algorithm.
	We’re still debugging the quartic equation solver it needs, so we approximated by fixing the phone’s starting location. Our final solution will be much less jittery, but we wanted to try out some visuals, including a teapot in a stadium, a teddy bear, and more…
	</p>
	
<h3 align="middle">Results</h3>	
    <p> We hope to be able to do this segmentation on every frame or maybe for every other frame. We hope to be able to see how much that would impact our accuracy as well.</p>

<h3 align="middle">References</h3>

	
<h3 align="middle">Contributions</h3>
<ol>
  <li>David: </li>
  <li>Ethan: </li>
  <li>James: </li>
  <li>Shilpa: </li>	
</ol>

</body>
</html>
